{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "#API KEY\n",
    "api_key = 'secret'\n",
    "\n",
    "#load the train.csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "#save the text data to a txt file\n",
    "with open('test.txt', 'w') as f:\n",
    "    for text in df['text']:\n",
    "        f.write(text + '.\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # You can choose a different model here\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Make sure to add the EOS token as PAD token to avoid warnings\n",
    "model.config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samdisorbo/Documents/code/python_projects/venv445/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Prepare the dataset and data collator\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"demosthenes_texts.txt\",\n",
    "    block_size=128)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./demosthenes_gpt2_finetuned\",  # Output directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,  # Number of training epochs, adjust as needed\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/462 [02:46<?, ?it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/616 [06:44<01:33,  1.23it/s]\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/616 [06:44<01:33,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8309, 'learning_rate': 9.415584415584416e-06, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 616/616 [08:18<00:00,  1.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 616/616 [08:18<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 498.4792, 'train_samples_per_second': 4.927, 'train_steps_per_second': 1.236, 'train_loss': 5.753316507711039, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=616, training_loss=5.753316507711039, metrics={'train_runtime': 498.4792, 'train_samples_per_second': 4.927, 'train_steps_per_second': 1.236, 'train_loss': 5.753316507711039, 'epoch': 4.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model to\n",
    "model.save_pretrained('model_gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the model\n",
    "model = GPT2LMHeadModel.from_pretrained('demosthenes_gpt2_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a conversational bot\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "#model path\n",
    "model_path = 'demosthenes_gpt2_finetuned'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How is the weather today?\"  # Example user input\n",
    "input_ids = tokenizer.encode(user_input, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of tokens in response to the input\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is the weather today?\n",
      "athenian sent ambassador athens\n",
      "athenian sent ambassador athens send ambassador athens send ambassador athens send ambassador athens send ambassador athens send ambassador athens send ambassador athens send ambassador athens\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  Helloaye man die right man make haste think rightly judge right act measure rightly think ill man justice just act rightly regard whether ought done wrong\n",
      "sadley r timoleon athenian general spade b war le tisquently seen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  What is your name?chiseledius general\n",
      "apprehend merest complaint athenian\n",
      "see ezretelus speaks without word meaning oration\n",
      "dia nobis alcion nomen\n",
      "conquering ship taken one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  You suckeotens manger rhodes isocrates king amyntas\n",
      "say however perhaps oleryric mean may say mean thebe\n",
      "must suppose thebe neither ariobarzanes amyntas orcha ardacus\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=50, \n",
    "        num_return_sequences=1, \n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        temperature=1.5, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        do_sample=True\n",
    "    )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Bot: \", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv445",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
